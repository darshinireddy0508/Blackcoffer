{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Necessary Libraries**\n",
        "\n",
        "In this cell, we are installing the required Python libraries for our data extraction and analysis tasks. These libraries include BeautifulSoup for web scraping, requests for handling HTTP requests, nltk for natural language processing, pandas for data manipulation, and openpyxl for working with Excel files. Run the following command to install these dependencies:"
      ],
      "metadata": {
        "id": "jRTF0UJvvWRH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FXYwqVlV34q",
        "outputId": "305bff01-5425-4c48-e0de-843f1f5e0712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install beautifulsoup4 requests nltk pandas openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extracting Articles from URLs**\n",
        "\n",
        "Here, we import the necessary libraries and define the functions for extracting article text from the given URLs listed in an Excel file.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "* Import Libraries: Import the required libraries for HTTP requests,\n",
        "HTML parsing, and data handling.\n",
        "* Load URLs: Read the URLs from the input Excel file using pandas.\n",
        "* Create Directory: Ensure a directory exists for storing the extracted articles.\n",
        "* Fetch Articles: Define a function to fetch and parse the article content from each URL.\n",
        "* Save Articles: Iterate through the URLs, fetch the article text, and save them to text files in the designated directory.\n",
        "\n",
        "The articles are saved in the articles directory with filenames based on their URL IDs."
      ],
      "metadata": {
        "id": "lTHWOOGivl27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.styles import Font\n",
        "\n",
        "# Load the URLs from the input Excel file\n",
        "input_file = '/content/Input.xlsx'\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a directory for the extracted articles\n",
        "os.makedirs('articles', exist_ok=True)\n",
        "\n",
        "# Function to fetch and extract article text\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Extract title and article text\n",
        "        title = soup.find('title').get_text()\n",
        "        article_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
        "        return title, article_text\n",
        "    return None, None\n",
        "\n",
        "# Iterate over the URLs and save the articles\n",
        "for idx, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    title, article_text = fetch_article_text(url)\n",
        "    if title and article_text:\n",
        "        with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"{title}\\n\\n{article_text}\")\n",
        "\n",
        "print(\"Articles have been successfully extracted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBk1OWu9V5MB",
        "outputId": "640c8e81-4dc0-40a8-c5a0-039d22419e99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles have been successfully extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Analysis and Calculation of Variables**\n",
        "\n",
        "This cell is responsible for loading the extracted articles, performing text analysis, and calculating various linguistic metrics.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "* Load Stop Words: Read stop words from multiple text files and combine them into a single set.\n",
        "* Load Positive and Negative Words: Load lists of positive and negative words.\n",
        "* Define Text Cleaning Function: A function to clean and tokenize the text, removing stop words and non-alphabetic tokens.\n",
        "* Calculate Variables: Define a function to compute various text analysis metrics such as word count, sentence count, positivity/negativity scores, fog index, and more.\n",
        "* Process Articles: Iterate through the saved article files, calculate the defined variables, and store the results.\n",
        "Merge Results with Output Structure: Merge the calculated results with a predefined output structure from an Excel file.\n",
        "* Save Results: Save the updated output to a new Excel file and format the URLs as clickable hyperlinks.\n",
        "\n",
        "The output is saved as 'Output Data Structure Updated.xlsx'."
      ],
      "metadata": {
        "id": "7qJQ-73jwBZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words from text files\n",
        "stop_words = set()\n",
        "stop_word_files = ['/content/StopWords_Auditor.txt', '/content/StopWords_Currencies.txt', '/content/StopWords_DatesandNumbers.txt', '/content/StopWords_Generic.txt', '/content/StopWords_GenericLong.txt', '/content/StopWords_Geographic.txt', '/content/StopWords_Names.txt']\n",
        "\n",
        "for file_path in stop_word_files:\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        stop_words.update(file.read().split())\n",
        "\n",
        "# Load positive and negative words\n",
        "with open('/content/positive-words.txt', 'r', encoding='latin-1') as file:\n",
        "    positive_words = set(file.read().split())\n",
        "\n",
        "with open('/content/negative-words.txt', 'r', encoding='latin-1') as file:\n",
        "    negative_words = set(file.read().split())\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Function to calculate variables\n",
        "def calculate_variables(text):\n",
        "    tokens = clean_text(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    word_count = len(tokens)\n",
        "    sentence_count = len(sentences)\n",
        "    complex_words = [word for word in tokens if len(word) > 2]  # Simplified condition for complex words\n",
        "    complex_word_count = len(complex_words)\n",
        "    syllables_per_word = sum(len([char for char in word if char in 'aeiou']) for word in tokens) / word_count\n",
        "\n",
        "    positive_score = sum(1 for word in tokens if word in positive_words)\n",
        "    negative_score = sum(1 for word in tokens if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
        "    avg_sentence_length = word_count / sentence_count\n",
        "    percentage_of_complex_words = complex_word_count / word_count\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
        "    avg_words_per_sentence = word_count / sentence_count\n",
        "    personal_pronouns = sum(1 for word in tokens if word in [\"i\", \"we\", \"my\", \"ours\", \"us\"])\n",
        "    avg_word_length = sum(len(word) for word in tokens) / word_count\n",
        "\n",
        "    return {\n",
        "        'POSITIVE SCORE': positive_score,\n",
        "        'NEGATIVE SCORE': negative_score,\n",
        "        'POLARITY SCORE': polarity_score,\n",
        "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "        'PERCENTAGE OF COMPLEX WORDS': percentage_of_complex_words,\n",
        "        'FOG INDEX': fog_index,\n",
        "        'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
        "        'COMPLEX WORD COUNT': complex_word_count,\n",
        "        'WORD COUNT': word_count,\n",
        "        'SYLLABLE PER WORD': syllables_per_word,\n",
        "        'PERSONAL PRONOUNS': personal_pronouns,\n",
        "        'AVG WORD LENGTH': avg_word_length\n",
        "    }\n",
        "\n",
        "# Load the extracted articles and calculate the variables\n",
        "results = []\n",
        "article_dir = 'articles'\n",
        "for file_name in os.listdir(article_dir):\n",
        "    if file_name.endswith('.txt'):\n",
        "        url_id = file_name.replace('.txt', '')\n",
        "        with open(os.path.join(article_dir, file_name), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            title, article_text = text.split('\\n\\n', 1)\n",
        "            variables = calculate_variables(article_text)\n",
        "            result = {\n",
        "                'URL_ID': url_id,\n",
        "                'URL': df[df['URL_ID'] == url_id]['URL'].values[0]\n",
        "            }\n",
        "            result.update(variables)\n",
        "            results.append(result)\n",
        "\n",
        "# Load the predefined output structure\n",
        "output_structure_file = '/content/Output Data Structure.xlsx'\n",
        "output_df = pd.read_excel(output_structure_file)\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Merge the results with the output structure\n",
        "final_df = pd.merge(output_df, results_df, on=['URL_ID', 'URL'], how='left')\n",
        "\n",
        "# Save the updated output file\n",
        "final_df.to_excel('/content/Output Data Structure Updated.xlsx', index=False)\n",
        "\n",
        "# Save the updated output file with clickable URLs\n",
        "writer = pd.ExcelWriter('/content/Output Data Structure Updated.xlsx', engine='openpyxl')\n",
        "final_df.to_excel(writer, index=False)\n",
        "\n",
        "workbook = writer.book\n",
        "worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "# Convert URLs into clickable hyperlinks\n",
        "for idx, row in final_df.iterrows():\n",
        "    url_cell = worksheet.cell(row=idx+2, column=2)  # URL column is the 2nd column\n",
        "    url = row['URL']\n",
        "    url_cell.hyperlink = url\n",
        "    url_cell.font = Font(color=\"0000FF\", underline=\"single\")\n",
        "\n",
        "writer.close()\n",
        "\n",
        "print(\"Textual analysis has been completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcr_5XV0WzC2",
        "outputId": "c4e5581f-c2a1-4f8a-ccfd-75059457bb06"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textual analysis has been completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download the Updated Output File**\n",
        "This final cell provides functionality to download the updated Excel file containing the results of the text analysis.\n",
        "\n",
        "Steps:\n",
        "\n",
        "* Import Colab Files Module: Import the module required for file download.\n",
        "* Download File: Execute the download function to allow the user to download the updated Excel file.\n",
        "* I manually adjusted minor features of the excel sheet like column width,Null values etc."
      ],
      "metadata": {
        "id": "ALKimUeKwkIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/Output Data Structure Updated.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gPqiBAXgbncH",
        "outputId": "e9813869-944a-41e5-a6c2-a5ae8746963d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6188bd79-cc59-429e-b7cf-119dd694afda\", \"Output Data Structure Updated.xlsx\", 28115)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dependencies Required**\n",
        "* beautifulsoup4\n",
        "* requests\n",
        "* nltk\n",
        "* pandas\n",
        "* openpyxl"
      ],
      "metadata": {
        "id": "-oLGqIrYw3f4"
      }
    }
  ]
}